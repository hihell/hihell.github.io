<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
 
 <title>Sirupsen</title>
 <link href="http://sirupsen.github.com/atom.xml" rel="self"/>
 <link href="http://sirupsen.github.com/"/>
 <updated>2013-07-12T21:58:39+08:00</updated>
 <id>http://sirupsen.github.com</id>
 <author>
   <name>Sirupsen</name>
   <email>sirup@sirupsen.com</email>
 </author>

 
 <entry>
   <title>music fingerprinting</title>
   <link href="http://sirupsen.github.com/music-fingerprinting"/>
   <updated>2013-07-10T07:31:51+08:00</updated>
   <id>http:/sirupsen.github.com/music-fingerprinting</id>
   <content type="html">&lt;h1 id=&quot;section&quot;&gt;音乐指纹 - 算法的框架&lt;/h1&gt;

&lt;p&gt;如何让电脑听歌辩曲呢？这里用到了music fingerprint的算法。所谓fingerprint就是找到一个能代表曲子的东西，就像指纹能代表一个人一样。&lt;/p&gt;

&lt;h2 id=&quot;shazam&quot;&gt;Shazam&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://play.google.com/store/apps/details?id=com.shazam.android&amp;amp;feature=search_result#?t=W251bGwsMSwxLDEsImNvbS5zaGF6YW0uYW5kcm9pZCJd&quot;&gt;Shazam-android&lt;/a&gt; | &lt;a href=&quot;https://itunes.apple.com/cn/app/shazam/id284993459?l=en&amp;amp;mt=8&quot;&gt;Shazam-iOS&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Shazam这个服务虽然是闭源的，但是有&lt;a href=&quot;http://111.13.109.34:82/1Q2W3E4R5T6Y7U8I9O0P1Z2X3C4V5B/www.ee.columbia.edu/~dpwe/papers/Wang03-shazam.pdf&quot;&gt;论文&lt;/a&gt;&lt;/p&gt;

&lt;h5 id=&quot;section-1&quot;&gt;指纹生成:&lt;/h5&gt;

&lt;p&gt;从生谱图(spectrogram)生成星座图(constellation)，所谓星座图就是在时间轴上取一些点（密度大概1秒10个），然后取在这些时间点上的最高能量的频率的能量(定一个时间点，一定会有很多频率有声音，选声音最大的那个频率)，星座图中的点代表该时间该频率的能量&lt;/p&gt;

&lt;p&gt;从时间纬度扫这个星座图，将点与后面target area里面的一堆点配对(具体target area怎么定论文没说)，每一对成为一个元素放入hash表，key是该时间和开始时间的距离，value就是两个点的频率和时间差(图)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/static/images/music_fingerprint_1/hash.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h5 id=&quot;section-2&quot;&gt;匹配和选择:&lt;/h5&gt;

&lt;p&gt;手机捕捉的声音片段也用同样的方法做指纹，将这些元素和服务器中的数据作比较(比较hash的value)，相同的按歌曲名字放在不同的桶里面，这个时候桶中的所谓match其实不一定是真的match，有可能是匹配到了错误的位置，解决的方案就是在一个以服务器样本时间和手机样本时间为坐标的坐标系中找斜线。如果手机捕获的片段确实是服务器中歌曲的一部分，那么这条线会很明显，否则就非常稀疏或者干脆找不到。找到最符合这个标准的桶，我们就有了匹配到的歌曲。&amp;lt;/br&amp;gt;&lt;/p&gt;

&lt;p&gt;以下分别是&lt;em&gt;匹配&lt;/em&gt;和&lt;em&gt;不匹配&lt;/em&gt;的样子&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/static/images/music_fingerprint_1/match.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/static/images/music_fingerprint_1/notmatch.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;以上就是Shazam的指纹算法了&lt;/p&gt;
&lt;hr /&gt;

&lt;h2 id=&quot;echoprint&quot;&gt;Echoprint&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/echonest/echoprint-codegen&quot;&gt;echoprint-code&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;这个没有现成的服务，有一些sample比如&lt;a href=&quot;https://github.com/echonest/echoprint-ios-sample&quot;&gt;ios-sample&lt;/a&gt;，&lt;a href=&quot;echoprint.me&quot;&gt;echoprint.me&lt;/a&gt;是他们的博客&lt;/p&gt;

&lt;p&gt;echoprint的算法和上面的就差很多了，感觉也更复杂一些，这里有&lt;a href=&quot;http://ismir2011.ismir.net/latebreaking/LB-7.pdf&quot;&gt;论文&lt;/a&gt;&lt;/p&gt;

&lt;h5 id=&quot;section-3&quot;&gt;指纹生成:&lt;/h5&gt;

&lt;p&gt;从生谱图找到八个频段的高能点，这里和星座图不一样(就叫带状图好了)，对于每个频段纪录能量大的点，但是只有时间，没有频率，也就是说输出的矩阵一维是时间，另一维就是8个频段的标号。每个元素代表该时间，该频段所有频率的最高能量&lt;/p&gt;

&lt;p&gt;指纹算法希望能通过用阈值(每个频段独立)过滤带状图得到一系列的能量较高的点，而且这些点和前后点的相邻距离又不会太近或者太远。这里引入IOI的概念(inter-onset-interval)，所谓IOI就是某个频段里面相邻的两个点的时间差。函数adaptiveOnset基本干的就是这个事情。&lt;/p&gt;

&lt;p&gt;控制IOI长度的方法又两个，一个是变化阈值，另一个是硬性规定长度。&lt;/p&gt;

&lt;p&gt;控制阈值就是如果上次能量超过了阈值，阈值更新为该能量x1.05。如果没有超过，阈值 x= e^(-1/t) t是个变量，如果超过一定时间还没有超过阈值的能量，t就减少(阈值随之减少)，否则t增加(阈值随之增加)。所以基本上就是如果IOI变短，那算法就增加阈值。如果IOI变长，算法就减少阈值。&lt;/p&gt;

&lt;p&gt;硬性规定最小长度就是如果本次命中和上次命中太近，就把上次命中覆盖掉。&lt;/p&gt;

&lt;p&gt;除了上面的逻辑，算法还会避免一些过高的能量，在上次没超阈值的情况下本次超过阈值会改变阈值，但是不会直接纪录该点，如果下一个点小于阈值，会纪录下来(代码中contact和lcontact干的是这个事情)。最后成为一个矩阵。&lt;/p&gt;

&lt;p&gt;下面就可以将IOI配对组成hash了，这个工作是在函数Compute中完成的，每个频段的每个点找到接下来的4个点，生成时间差(IOI)，前后相邻的IOI组成配对，生成6个对(看图)，把这些对顺序放进一个数据结构中，成为指纹。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/static/images/music_fingerprint_1/pairs.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h5 id=&quot;section-4&quot;&gt;匹配和选择:&lt;/h5&gt;
&lt;p&gt;服务端的样本分析是把歌曲拆成60秒的片段进行的，相邻片段有50%重叠(看图)，目的是为了防止时长比较高的歌会有更高的得分，同时防止跨界的样本损失太多&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/static/images/music_fingerprint_1/windows.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;在一个直方图里纪录hash的匹配次数，直方图一个维度是匹配次数，另一个维度是hash在歌曲中发生的时间&lt;/p&gt;

&lt;p&gt;比较所有hash(手机的样本和服务器的)，如果吻合就按照时间在直方图中累加，比较完成后选取累计次数最多的那个值作为该片段的得分，最后返回得分最高的。和Shazam的方法一样，这种hash的匹配会有很大几率配到错的位置上，但是如果样本和服务器的文件真的匹配，时差会很集中，也就是直方图上的那个突出的值会很大。&lt;/p&gt;

&lt;p&gt;以上就是echoprint-codegen的算法了&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>idea memo 1</title>
   <link href="http://sirupsen.github.com/idea-memo-1"/>
   <updated>2013-06-27T07:31:51+08:00</updated>
   <id>http:/sirupsen.github.com/idea-memo-1</id>
   <content type="html">&lt;p&gt;如何判断一个人能不能在短期内能吸取教训，并且不再犯同样的错误&lt;/p&gt;

&lt;p&gt;建立一个音乐库，标明自己是怎么发现这首乐曲的&lt;/p&gt;

&lt;p&gt;豆瓣关系，为什么开始follow一个人，可以查看我都因为什么原因开始follow各种人的，我第一次看到对方是在什么地方&lt;/p&gt;

&lt;p&gt;一个服务人员的评价平台，企业招聘服务人员的时候可以知道之前顾客是怎么评价这个人的，是不是有服务意识，服务人员也可以积累reputation，同样的，所有服务员有统一标识，方便评价－好像有点儿不人道－服务员有自由选择是否加入这个系统&lt;/p&gt;

&lt;p&gt;豆瓣的品味成就系统，直观显示一个人对虚拟社区的贡献值&lt;/p&gt;

&lt;p&gt;像往纸质读物上加标注一样简单的在网页上加标注&lt;/p&gt;

&lt;p&gt;关于进度条的想法
进度条不是给人类用的，就像让用户考虑内存占用问题一样
人对事件的记忆远远大于对时间进度的记忆，应该用时间表明主要点，然后用户可以选择在点与点之间的大概位置
需要算法自动识别人类对事件的定义&lt;/p&gt;

&lt;p&gt;Chrome可以有类似Apple Preview的搜索功能，如果输入关键词不会跳转，而是显示所有match的列表(现在一旦有匹配马上跳到第一个，这时候关键词可能还没输入完全，如果网页很长就会费很大功夫找之前的位置)&lt;/p&gt;

&lt;p&gt;一个能吧五线谱翻译成对小提琴更直接的谱子的算法，包括换弦和换把的优化&lt;/p&gt;

&lt;p&gt;由于音乐和歌曲很多（比电影多多了），但是搜索和机器推荐音乐目前有不靠谱，靠音乐社区这种人和人之间的推荐和传递也许是低成本发现新的自己喜欢的音乐的一个很好的途径&lt;/p&gt;
</content>
 </entry>
 
</feed>
